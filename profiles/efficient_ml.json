{
  "name": "efficient_ml",
  "topics": [
    "efficient machine learning and model compression",
    "neural network quantization and pruning techniques",
    "hardware-aware deep learning optimization",
    "efficient inference for large language models"
  ],
  "keywords": [
    "quantization",
    "INT8",
    "INT4",
    "pruning",
    "sparsity",
    "LoRA",
    "adapters",
    "MoE",
    "mixture of experts",
    "FlashAttention",
    "efficient attention",
    "GPU optimization",
    "edge deployment",
    "latency",
    "knowledge distillation",
    "model compression"
  ],
  "past_papers": [
    {
      "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
      "abstract": "Large language models require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance.",
      "arxiv_id": null
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "abstract": "We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.",
      "arxiv_id": null
    },
    {
      "title": "MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models",
      "abstract": "As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.",
      "arxiv_id": "2601.11464v1"
    }
  ],
  "preferred_authors": [
    ""
  ]
}